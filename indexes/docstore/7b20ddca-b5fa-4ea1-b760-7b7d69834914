{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"doc_id": "7b20ddca-b5fa-4ea1-b760-7b7d69834914"}, "page_content": "3.5 Usage patterns by model type\n\nAs AI capabilities evolve, understanding how different models are used can help anticipate shifts in usage across occupations. We explore this by comparing usage patterns between two Claude models: Claude 3 Opus, released in March 2024, and Claude 3.5 Sonnet (new), released in October 2024.\n\nOur analysis reveals clear specialization in how these models are used (Figure 8). Relative to Sonnet, Opus sees higher usage for creative and educational work (e.g., \"Produce and perform in film, TV, theater, and music,\" \"Manage book and document publishing process,\" \"Design and develop comprehensive educational curricula and materials,\" \"Conduct academic research and disseminate findings\"). These patterns align with widespread user observations about Opus\u2019 relatively unique character and writing style.7 By contrast, Claude 3.5 Sonnet (new) is preferred for coding and software development tasks (e.g., \"Develop and maintain software applications and websites\" and \"Program and debug computer systems and machinery\"), consistent with external evaluations highlighting its relatively strong coding abilities.\n\nBy tracking these usage patterns across model versions at a task-level, we can better understand which capability improvements drive meaningful changes in AI usage across different sectors of the economy.\n\n4 Discussion\n\nWe present the first large-scale empirical analysis of how advanced AI systems are actually being used across economic tasks. While our work offers broad insights on AI\u2019s use in the economy, we note key limitations and areas for future research.\n\n7https://www.anthropic.com/research/claude-character\n\n11\n\n4.1 Limitations\n\nData sample We use snapshots of Claude.ai Free and Pro conversations over 7-day periods.8 It is possible that this sample is not representative of usage on Claude.ai across longer time windows, and quite likely that our sample differs in important ways from API data or data from other AI model providers due to differing model capabilities, product features, and user bases. Additionally, Claude.ai only outputs text as opposed to other modalities. This removes key potential users who may rely on image or video outputs (e.g. fashion designers). For these reasons our findings should be interpreted as an imperfect snapshot of AI usage across the labor market, while noting that a broader understanding of model interaction patterns will emerge as more researchers and organizations are able to share usage data from diverse deployment contexts.\n\nReliability of model-driven classification Our use of Claude to classify user conversations may also introduce potential inconsistencies if the model\u2019s understanding of tasks differs from the intended reading in the O*NET database. While we conduct human validation of our pipeline (Appendix C), rely upon past validations of Clio [Tamkin et al., 2024], and corroborate our results with cluster-level analyses (Appendix G), it is important to note that these classifications likely contain some inherent noise.\n\nVarying complexity of users\u2019 queries While we make efforts to exclude conversations without relevance to any occupational tasks (Appendix B), our method does not account for the complexity of user queries\u2014for example, providing instructions for a basic omelette does not indicate culinary ex- pertise. Consequently, we may overestimate usage rates for certain tasks by classifying conversations from novice users.\n\nLimitations of the O*NET database While the O*NET database offers valuable insights into current economic sectors, its static nature presents key limitations for analyzing AI\u2019s impact on the labor market. The database cannot capture emerging tasks and occupations that AI systems such as Claude may create or transform. Additionally, while O*NET captures a very large number of tasks, it cannot contain all tasks in the economy. Furthermore, these tasks are often written in general terms, leading to inherent ambiguity when classifying conversations\u2014many tasks are similar across multiple different occupations. Finally, as a U.S.-centric classification system, O*NET may overlook significant occupational categories and tasks from other regions, potentially skewing our distributional analysis of global Claude.ai usage. This limits our analysis as AI usage patterns can vary across international contexts Gmyrek et al. [2023].\n\nLack of full context into user workflows Although our work analyzes conversation data on Claude.ai, our methods are not able to capture how users are using the outputs of Claude.ai conversa- tions. For example, we cannot observe whether users are copying code snippets into development environments, incorporating writing suggestions into documents, fact-checking responses against other sources, or using outputs as inspiration rather than verbatim content. Thus, it remains out of reach to make definitive judgments as to how much Claude\u2019s outputs are actually incorporated by users in their tasks. We aim to provide the preliminary framework and findings for this further study to take place.", "type": "Document"}}