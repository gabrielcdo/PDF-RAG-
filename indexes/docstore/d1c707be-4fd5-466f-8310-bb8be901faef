{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"doc_id": "d1c707be-4fd5-466f-8310-bb8be901faef"}, "page_content": "C Human Validation\n\nWe conduct human validation studies on a subset of conversations to assess the quality of our classification framework. To conduct these studies, we used conversations for which users submitted feedback from Claude.ai Free and Pro. We then used Clio to analyze these conversations. Consistent with our Privacy Policy, which allows us to access feedback data for research purposes, we retained the ability to view the underlying user-feedback conversations in the resulting Clio clusters. For more information, please see Anthropic\u2019s relevant FAQs and privacy policy.\n\nWhile we acknowledge the distribution of conversations for which users submitted feedback likely differs from regular Claude.ai usage, we consider this distribution to be more reliable than alternative open-source datasets.\n\nTask hierarchy We hand-validate 150 examples for our task hierarchy classifications (Section 3.1 and Appendix B.1). We examine the classification path traversed by each conversation through our hierarchical framework. We assess the accuracy of labels assigned at each level of the hierarchy:\n\n\u2022 At the top level, 95.3% of conversations are judged as assigned to an acceptable task.\n\n\u2022 At the middle level, 91.3% of conversations are judged as assigned to an acceptable task.\n\n\u2022 At the base (O*NET) level, 86% of conversations are judged as assigned to an acceptable task.\n\nAs tasks become more specific, classification becomes more difficult. Thus, accuracy of our method predictably decreases as we traverse the hierarchy. Furthermore, there is likely some irreducible noise in the assignment process, given that the O*NET database, while extensive, does not encompass all possible tasks in the economy. As model capabilities increase, we expect classification performance to improve; we treat this as the baseline performance for future efforts to track AI usage using our framework. Although our methods are imperfect, our high validation scores provide confidence in our overall methodology and results.\n\nAutomating vs. augmenting users We hand-validate 150 examples for our automation vs. aug- mentation classifications (Section 3.4). We assess whether the label assigned by our framework (Directive, Feedback Loop, Validation, Task Iteration, or Learning) represents the best categorization for each conversation. We find that 90.7% of conversations are assigned to their optimal label as assessed by human raters.", "type": "Document"}}